
The Versatility of Learning
===========================

The purpose of this tutorial is to demonstrate the versatility of
Shark for various learning tasks. Drawing on a simple binary
classification task similar to the one in the
:doc:`../../first_steps/hello_shark` tutorial, we will cover five
different learning methods in a single, consistent framework.  The
present tutorial assumes that the reader is already familiar with the
concepts of models and trainers that have been treated in more detail,
e.g., in the :doc:`../../first_steps/general_optimization_tasks`
tutorial.

We will start out with the (hopefully already familiar) structure of a
supervised learning experiment: ::

..sharkcode<Supervised/VersatileClassificationTutorial-LDA.tpp,skeleton>

The program assumes the comma-separated-value (csv) file ``quickstartData.csv``
with training and test data located in the sub-folder ``/data``. The
file content describes a two-class (binary) problem, with
labels 0 and 1. The program itself is still a stub, since the actual
model and trainer declarations are missing. The ``ZeroOneLoss``
computes the classification error. It replaces the loop at the end of
the :doc:`../../first_steps/hello_shark` tutorial.


Many Ways of Classifying Data
=============================

In the following we will demonstrate the versatility of Shark by
inserting five different learning methods into the above program
structure. In passing we will see how to circumvent typical pitfalls.


Linear Discriminant Analysis
----------------------------

Let us start with a classical linear method, namely linear discriminant
analysis (LDA). We need two more includes ::

..sharkcode<Supervised/VersatileClassificationTutorial-LDA.tpp,LDA-includes>

and in the place of the "TODO" comment we insert ::

..sharkcode<Supervised/VersatileClassificationTutorial-LDA.tpp,LDA>

That's it! The program is ready to go. For build instructions refer to
the :doc:`../../first_steps/your_programs` tutorial.  You can learn
more on LDA in the :doc:`../../algorithms/lda` tutorial.


Nearest Neighbor Classifier
---------------------------

Let's move from the linear parametric LDA approach to a non-linear,
non-parametric approach.
The arguably simplest non-linear classifier is the nearest neighbor classifier.
This classifier is special in that it does not require a trainer. Let's
remove the LDA code and insert the following code in the appropriate
places: ::

..sharkcode<Supervised/VersatileClassificationTutorial-NN.tpp,NN-includes>
..sharkcode<Supervised/VersatileClassificationTutorial-NN.tpp,NN>

For the time being ignore the helper classes, unless you already know what
they do. There is no explicit training step for nearest neighbor prediction
and therefore no trainer object. So we remove the lines::

..sharkcode<Supervised/VersatileClassificationTutorial-LDA.tpp,remove>

Everything should work right away. For more information on nearest neighbor
classification see the :doc:`../../algorithms/nearestNeighbor` tutorial.

You see, changing the learning method is really easy.
So let's try more.


Support Vector Machine
----------------------

Our next candidate is a non-linear support vector machine (SVM). We will
use a Gaussian radial basis function kernel: ::

..sharkcode<Supervised/VersatileClassificationTutorial-SVM.tpp,SVM-includes>
..sharkcode<Supervised/VersatileClassificationTutorial-SVM.tpp,SVM>

Quite simple, again. That's it; you are ready to enjoy the power of
non-linear SVM classification. Much more on SVMs can be found in the
special SVM tutorials, starting with :doc:`../../algorithms/svm`.


Random Forest
-------------

There is more to explore in Shark. Let's try a random forest instead: ::

..sharkcode<Supervised/VersatileClassificationTutorial-RF.tpp,RF-includes>
..sharkcode<Supervised/VersatileClassificationTutorial-RF.tpp,RF>

This one is really straightforward. For an introduction to random forests see the
:doc:`../../algorithms/rf` tutorial.

However, the attempt to compile this program results in an error message
(or, depending on your compiler, a pile of hard-to-decrypt messages
involving template issues). What went wrong?
The problem is that in Shark there exist (for good reasons) two
different conventions for representing classification labels and
predictions (also refer to the :doc:`../data/labels` tutorial). While
many output their prediction as unsigned integers, the RFClassifier outputs
a RealVector holding one value per class. Here it contains two values, the
higher of which indicates its prediction. Thus, we have to turn the line ::

..sharkcode<Supervised/VersatileClassificationTutorial-LDA.tpp,int-prediction>

into ::

..sharkcode<Supervised/VersatileClassificationTutorial-RF.tpp,real-prediction>

Now predictions are stored as RealVectors. The next thing is that these
predictions are fed into the ZeroOneLoss. We change its definition into ::

..sharkcode<Supervised/VersatileClassificationTutorial-RF.tpp,real-loss>

where the first template parameter identifies the ground truth label
type (the type of test.label(n)) and the second template parameter is
the data type of model predictions (it can be dropped if the types
coincide).


Neural Network
--------------

As a final example let's look at a more complex case, namely that of
feed forward neural network training. The most basic way of training
these models is by gradient-based minimization of the training error
(empirical risk), measured by some differentiable loss function such
as the squared error or the cross entropy. The computation of the
gradient is built into the neural network class (back-propagation
algorithm), but of course there are various options for solving the
underlying optimization problem. The :doc:`../../first_steps/general_optimization_tasks`
tutorial touches this topic. Here - for consistency with the previous
examples - we will encapsulate the optimization process into the
familiar model and trainer classes. ::

..sharkcode<Supervised/VersatileClassificationTutorial-Network.tpp,Network-includes>
..sharkcode<Supervised/VersatileClassificationTutorial-Network.tpp,Network>

Just like for random forests, FFNet objects output RealVectors and
therefore must be used with the appropriate data containers and loss
functions.

The important classes here are ErrorFunction and OptimizationTrainer.
An ErrorFunction allows us to build an objective function that can be
interfaced by arbitrary optimization strategies from a model, a loss,
and data. The argument of the ErrorFunction is the parameter vector of
the model, and its evaluation computes the empirical risk of the model
measured by the provided loss function on the given data. This allows
for the definition of a general optimization procedure with an iterative
optimizer and a stopping condition. The OptimizationTrainer is a simple
wrapper class that keeps references to the objective function (usually
an ErrorFunction), the optimizer and the stopping condition and
implements a straightforward iterative optimization loop in its train
method. Feel free to use other (differentiable) loss functions for
training, other (usually gradient-based) optimizers, and different
stopping criteria. All this can be done without changing the program
structure. In particular, after all definitions have been made there
will always be a model and trainer, and that's all we need to care for
in the end.


What you learned
================

You should have learned the following aspects in this tutorial:

* Shark is a versatile tool for machine learning. Changing the learning method requires only exchanging a few classes. All objects still conform to the same top level interfaces, such as AbstractModel and AbstractTrainer.
* Nearly everything in Shark is templated. It is not always easy to get all template parameters right in the first attempt. The probably best way of dealing with errors is to check the documentation of the template classes. The meaning of all template parameters should be documented. Often it will also become clear from the template parameter's name.

You may not have understood all details, in particular those hidden in
the various helper classes. If you are particularly interested in one
of the methods then please feel encouraged go ahead and explore the
documentation.

In any case you should have understood how all the different learning
methods are expressed by means of adaptive models and corresponding
trainers. Changing the learning method may involve changing the
particular sub-class, but all relevant objects will still conform to
the same top-level interfaces. Thus, only minimal changes to the
surrounding code will be necessary, if any at all. This design offers
a lot of flexibility, since changing the learning algorithm even late
in a project is usually not a big deal.
